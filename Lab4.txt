#include <opencv2/opencv.hpp>
#include <opencv2/features2d.hpp>
#include "Timer.h"

using namespace cv;
using namespace std;


cv::Mat translationMatrix(double dx, double dy) {
	cv::Mat T = cv::Mat::eye(3, 3, CV_64F);
	T.at<double>(0, 2) = dx;
	T.at<double>(1, 2) = dy;
	return T;
}

int main(int argc, char* argv[]) {

	Timer myTimer;

	//read images
	cv::Mat image1 = cv::imread("image1.jpg");
	cv::Mat image2 = cv::imread("image2.jpg");

	//create sift pointer and use it to detect features
	Ptr<SIFT> sift = SIFT::create();
	std::vector<cv::KeyPoint> keypoints1;
	cv::Mat descriptors1;
	sift->detectAndCompute(image1, cv::noArray(), keypoints1, descriptors1);
	std::cout << "Found " << keypoints1.size() << " features" << std::endl;

	//use the sift pointer to detect features
	std::vector<cv::KeyPoint> keypoints2;
	cv::Mat descriptors2;
	sift->detectAndCompute(image2, cv::noArray(), keypoints2, descriptors2);
	std::cout << "Found " << keypoints2.size() << " features" << std::endl;


	if (image1.empty()) {
		std::cerr << "Could not load image from image1.jpg" << std::endl;
		return -1;
	}
	if (image2.empty()) {
		std::cerr << "Could not load image from image2.jpg" << std::endl;
		return -1;
	}

	//creating Mats to hold the two images with features overlayed side by side
	cv::Mat kptImage1;
	cv::drawKeypoints(image1, keypoints1, kptImage1, cv::Scalar(0, 255, 0),
		cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS);

	cv::Mat kptImage2;
	cv::drawKeypoints(image2, keypoints2, kptImage2, cv::Scalar(0, 255, 0),
		cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS);

	//matcher algorithim to compare the features detected in the image
	//better matcher algorithim
	cv::Ptr<cv::DescriptorMatcher> matcher1 = cv::FlannBasedMatcher::create();

	//reset timer to 0
	myTimer.reset();

	//create a vector to store the matches, here we have a list of 
	std::vector<std::vector<cv::DMatch>> matches1;

	//find matches in the features
	//knnMatches are finding k Nearest Neighbor matches
	matcher1->knnMatch(descriptors1, descriptors2, matches1, 2);

	//read timer output
	double elapsedTime = myTimer.read();
	std::cout << "++++++++ That code took " << elapsedTime << " seconds +++++++" << std::endl;

	//this list will hold the feature point locations
	std::vector<cv::Point2f> goodPts1, goodPts2;


	//choose only the matches whose matches are significatly better than the second best matches
	std::vector<cv::DMatch> goodMatches;
	for (const auto& match : matches1) {
		if (match[0].distance < 0.8 * match[1].distance) {
			goodMatches.push_back(match[0]);

			//names are a carry over from SIFTS original use as object recognition 
			//query points are to be matched to training points
			goodPts1.push_back(keypoints1[match[0].queryIdx].pt);
			goodPts2.push_back(keypoints2[match[0].trainIdx].pt);
		}
	}

	//compute homography
	std::vector<unsigned char> inliers;
	cv::Mat H = cv::findHomography(goodPts2, goodPts1, inliers, cv::RANSAC);

	//print homography
	std::cout << H << std::endl;

	//define the corners of image2
	cv::Mat c1(3, 1, CV_64F);
	c1.at<double>(0, 0) = 0;
	c1.at<double>(1, 0) = 0;
	c1.at<double>(2, 0) = 1;

	cv::Mat c2(3, 1, CV_64F);
	c2.at<double>(0, 0) = image2.size().width;
	c2.at<double>(1, 0) = 0;
	c2.at<double>(2, 0) = 1;

	cv::Mat c3(3, 1, CV_64F);
	c3.at<double>(0, 0) = image2.size().width;
	c3.at<double>(1, 0) = image2.size().height;
	c3.at<double>(2, 0) = 1;

	cv::Mat c4(3, 1, CV_64F);
	c4.at<double>(0, 0) = 0;
	c4.at<double>(1, 0) = image2.size().height;
	c4.at<double>(2, 0) = 1;

	//vector of Mats to hold corner points
	std::vector<cv::Mat> corners = { c1, c2, c3, c4};

	//holds the corners that will be iterated over
	int TwoDPoints[8][2];

	//compute new coordinates to find their location in the image by multiplying by the computed Homography
	for (int i = 0; i < 4; i++) {
		//add the points on the matrix mutliplied image to the twoDPoints array
		TwoDPoints[i+4][0] = corners[i].at<double>(0, 0);
		TwoDPoints[i+4][1] = corners[i].at<double>(1, 0);

		//apply matrix multiplication
		corners[i] = H * corners[i];

		//add the matrix multiplied points to the twoDPoints array
		TwoDPoints[i][0] = corners[i].at<double>(0, 0);
		TwoDPoints[i][1] = corners[i].at<double>(1, 0);
	}

	//Now we need to find the min and max points
	int minX = TwoDPoints[0][0], minY = TwoDPoints[0][1];
	int maxX = TwoDPoints[0][0], maxY = TwoDPoints[0][1];

	// Iterate through the array to find min and max values
	for (int i = 0; i < 8; i++) {
		// Update minX and maxX
		if (TwoDPoints[i][0] < minX)
			minX = TwoDPoints[i][0];
		else if (TwoDPoints[i][0] > maxX)
			maxX = TwoDPoints[i][0];

		// Update minY and maxY
		if (TwoDPoints[i][1] < minY)
			minY = TwoDPoints[i][1];
		else if (TwoDPoints[i][1] > maxY)
			maxY = TwoDPoints[i][1];
	}

	//find what size the new frame must be
	double FrameSizeX = maxX - minX;
	double FrameSizeY = maxY - minY;

	//frame the image in a size that fits all the 
	cv::Mat frame(FrameSizeY, FrameSizeX, CV_8UC3);
	cv::Mat mosaic = image1.clone();

	//translation to put image1 in upper righthand conrner
	cv::Mat t1 = translationMatrix(-minX, -minY);
	//Homography warp matrix to put image1 in the image with no cahnges
	cv::Mat t2 = translationMatrix(0, 0);

	//warp the two images into the blank frame large enough to hold them  
	//the order of these translations was messing this up completely ie. t1*t2 and t2*t1 have totally differing effects 
	//took some tinkering to figure that out.
	cv::warpPerspective(image1, mosaic, t1*t2, frame.size(),
		cv::INTER_NEAREST, cv::BORDER_TRANSPARENT);

	cv::warpPerspective(image2, mosaic, t1*H, frame.size(),
		cv::INTER_NEAREST, cv::BORDER_TRANSPARENT);

	//dispay image
	cv::imshow("Image Mosaic", mosaic);
	cv::waitKey();

	return 0;
}
